{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/FoKB5Z5.png\" align=\"left\" width=\"300\" height=\"250\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Code: J620-002-4:2020 \n",
    "\n",
    "## Program Name: FRONT-END SOFTWARE DEVELOPMENT\n",
    "\n",
    "## Title :  Basic Text Analytics using NLTK\n",
    "\n",
    "#### Name: \n",
    "\n",
    "#### IC Number:\n",
    "\n",
    "#### Date :\n",
    "\n",
    "#### Introduction : \n",
    "\n",
    "\n",
    "\n",
    "#### Conclusion :\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Basic Text Analytics \n",
    "\n",
    "- Word & Sentence tokenizer\n",
    "\n",
    "- Parts of Speech (POS) tagger\n",
    "\n",
    "- Extracting Entities\n",
    "\n",
    "**Installation**\n",
    "\n",
    "conda install -c conda-forge nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zheny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zheny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics for Beginners using NLTK\n",
    "Learn How to analyze text using NLTK. Analyze people's sentiments and classify movie reviews.\n",
    "\n",
    "In today's area of internet and online services, data is generating at incredible speed and amount. Generally, Data analyst, engineer, and scientists are handling relational or tabular data. These tabular data columns have either numerical or categorical data. Generated data has a variety of structures such as text, image, audio, and video. Online activities such as articles, website text, blog posts, social media posts are generating unstructured textual data. Corporate and business need to analyze textual data to understand customer activities, opinion, and feedback to successfully derive their business. To compete with big textual data, text analytics is evolving at a faster rate than ever before.\n",
    "\n",
    "Text Analytics has lots of applications in today's online world. By analyzing tweets on Twitter, we can find trending news and peoples reaction on a particular event. Amazon can understand user feedback or review on the specific product. BookMyShow can discover people's opinion about the movie. Youtube can also analyze and understand peoples viewpoints on a video.\n",
    "\n",
    "In this tutorial, you are going to cover the following topics:\n",
    "\n",
    "Text Analytics and NLP\n",
    "Compare Text Analytics, NLP and Text Mining\n",
    "Text Analysis Operations using NLTK\n",
    "Tokenization\n",
    "Stopwords\n",
    "Lexicon Normalization such as Stemming and Lemmatization\n",
    "POS Tagging\n",
    "Sentiment Analysis\n",
    "Text Classification\n",
    "Performing Sentiment Analysis using Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics and NLP\n",
    "\n",
    "Text communication is one of the most popular forms of day to day conversion. We chat, message, tweet, share status, email, write blogs, share opinion and feedback in our daily routine. All of these activities are generating text in a significant amount, which is unstructured in nature. I this area of the online marketplace and social media, It is essential to analyze vast quantities of data, to understand peoples opinion.\n",
    "\n",
    "NLP enables the computer to interact with humans in a natural manner. It helps the computer to understand the human language and derive meaning from it. NLP is applicable in several problematic from speech recognition, language translation, classifying documents to information extraction. Analyzing movie review is one of the classic examples to demonstrate a simple NLP Bag-of-words model, on movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Text Analytics, NLP and Text Mining\n",
    "\n",
    "Text mining also referred to as text analytics. Text mining is a process of exploring sizeable textual data and find patterns. Text Mining process the text itself, while NLP process with the underlying metadata. Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining. Natural language processing is one of the components of text mining. NLP helps identified sentiment, finding entities in the sentence, and category of blog/article. Text mining is preprocessed data for text analytics. In Text Analytics, statistical and machine learning algorithm used to classify information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis Operations using NLTK\n",
    "\n",
    "NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. It is free, opensource, easy to use, large community, and well documented. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. NLTK helps the computer to analysis, preprocess, and understand the written text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. Token is a single entity that is building blocks for sentence or paragraph.\n",
    "\n",
    "## Sentence Tokenization\n",
    "Sentence tokenizer breaks text paragraph into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Table & Apron – formerly The Kitchen Table Restaurant & Bakery – doesn’t exist to disrupt the scene.', 'From the outside, it barely stretches the boundaries of what is an already saturated restaurant-cum-bakery scene.', 'But none of it matters.', 'Because right from its birth in 2014, Table & Apron has proven to be a restaurant that has in spades a component so elementary yet so rare – heart.', 'Through hard work, dedication and all the boring old-fashioned virtues of an honest operation, owner Marcus Low and his team have carved for us a little treasure in Damansara Kim.', '(Credit must also be given to former co-owner Mei Wan Tan.)', 'The narcissism you’ll find in so many KL']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# conda install -c anaconda nltk\n",
    "\n",
    "url = 'https://www.timeout.com/kuala-lumpur/food-and-drink/food-reviews'\n",
    "\n",
    "text = '''Table & Apron – formerly The Kitchen Table Restaurant & Bakery – doesn’t exist to disrupt the scene. From the outside, it barely stretches the boundaries of what is an already saturated restaurant-cum-bakery scene. But none of it matters. Because right from its birth in 2014, Table & Apron has proven to be a restaurant that has in spades a component so elementary yet so rare – heart. Through hard work, dedication and all the boring old-fashioned virtues of an honest operation, owner Marcus Low and his team have carved for us a little treasure in Damansara Kim. (Credit must also be given to former co-owner Mei Wan Tan.) The narcissism you’ll find in so many KL \n",
    "'''\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the given text is tokenized into sentences.\n",
    "\n",
    "## Word Tokenization\n",
    "Word tokenizer breaks text paragraph into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Table', '&', 'Apron', '–', 'formerly', 'The', 'Kitchen', 'Table', 'Restaurant', '&', 'Bakery', '–', 'doesn', '’', 't', 'exist', 'to', 'disrupt', 'the', 'scene', '.', 'From', 'the', 'outside', ',', 'it', 'barely', 'stretches', 'the', 'boundaries', 'of', 'what', 'is', 'an', 'already', 'saturated', 'restaurant-cum-bakery', 'scene', '.', 'But', 'none', 'of', 'it', 'matters', '.', 'Because', 'right', 'from', 'its', 'birth', 'in', '2014', ',', 'Table', '&', 'Apron', 'has', 'proven', 'to', 'be', 'a', 'restaurant', 'that', 'has', 'in', 'spades', 'a', 'component', 'so', 'elementary', 'yet', 'so', 'rare', '–', 'heart', '.', 'Through', 'hard', 'work', ',', 'dedication', 'and', 'all', 'the', 'boring', 'old-fashioned', 'virtues', 'of', 'an', 'honest', 'operation', ',', 'owner', 'Marcus', 'Low', 'and', 'his', 'team', 'have', 'carved', 'for', 'us', 'a', 'little', 'treasure', 'in', 'Damansara', 'Kim', '.', '(', 'Credit', 'must', 'also', 'be', 'given', 'to', 'former', 'co-owner', 'Mei', 'Wan', 'Tan', '.', ')', 'The', 'narcissism', 'you', '’', 'll', 'find', 'in', 'so', 'many', 'KL']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Table', '&', 'Apron', '–', 'formerly', 'The', 'Kitchen', 'Table', 'Restaurant', '&', 'Bakery', '–', 'doesn', '’', 't', 'exist', 'to', 'disrupt', 'the', 'scene', '.', 'From', 'the', 'outside', ',', 'it', 'barely', 'stretches', 'the', 'boundaries', 'of', 'what', 'is', 'an', 'already', 'saturated', 'restaurant', '-', 'cum', '-', 'bakery', 'scene', '.', 'But', 'none', 'of', 'it', 'matters', '.', 'Because', 'right', 'from', 'its', 'birth', 'in', '2014', ',', 'Table', '&', 'Apron', 'has', 'proven', 'to', 'be', 'a', 'restaurant', 'that', 'has', 'in', 'spades', 'a', 'component', 'so', 'elementary', 'yet', 'so', 'rare', '–', 'heart', '.', 'Through', 'hard', 'work', ',', 'dedication', 'and', 'all', 'the', 'boring', 'old', '-', 'fashioned', 'virtues', 'of', 'an', 'honest', 'operation', ',', 'owner', 'Marcus', 'Low', 'and', 'his', 'team', 'have', 'carved', 'for', 'us', 'a', 'little', 'treasure', 'in', 'Damansara', 'Kim', '.', '(', 'Credit', 'must', 'also', 'be', 'given', 'to', 'former', 'co', '-', 'owner', 'Mei', 'Wan', 'Tan', '.)', 'The', 'narcissism', 'you', '’', 'll', 'find', 'in', 'so', 'many', 'KL']\n"
     ]
    }
   ],
   "source": [
    "# depending on domain, tokenizer\n",
    "print(nltk.wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between word_tokenize and wordpunct_tokenize\n",
    "\n",
    "Reference: https://stackoverflow.com/questions/50240029/nltk-wordpunct-tokenize-vs-word-tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 96 samples and 133 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist= FreqDist(words)\n",
    "\n",
    "print(fdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 6), ('the', 4), (',', 4)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30,cumulative=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Stopwords considered as noise in the text. Text may contain stop words such as is, am, are, this, a, an, the, etc.\n",
    "\n",
    "In NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of tokens from these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out', 'its', 'and', \"you've\", 'can', 'yourselves', 'under', 'y', \"shan't\", 'her', 'be', 't', 'doesn', \"don't\", 'shouldn', 'at', 'some', 'himself', 'over', 'was', 'me', 'itself', 'further', 'won', 'these', 'shan', 'his', 'after', 'of', 'between', 'had', 'down', 'off', 'our', 'again', 'yourself', 'am', 'up', 'each', 'weren', 'd', 'so', 'above', 'wasn', 'hers', 'where', 'aren', 'my', \"isn't\", \"aren't\", 'ain', 'haven', 'this', 'into', 'has', 'him', 'herself', 'the', 'until', \"she's\", \"won't\", 'by', 'few', 'mustn', 'on', 'here', 'a', 'just', 'with', 'only', 'as', 'mightn', \"you're\", 'against', 'did', 'how', 'theirs', \"haven't\", 'isn', 'their', \"that'll\", 'couldn', 'in', 'through', 'but', 'below', 're', \"you'd\", 'or', 'any', 'same', 'were', 'ourselves', 'yours', 'whom', \"weren't\", \"shouldn't\", 'i', 'who', 'that', 'have', 'm', 'for', 'too', 'if', 'been', 'needn', 'those', \"doesn't\", 'both', \"didn't\", 'we', \"wasn't\", 'not', 'while', 'there', 'such', 've', 'they', 'do', 'nor', \"needn't\", \"hadn't\", 'during', 'more', \"should've\", 'to', 'from', \"you'll\", 'hasn', 'what', \"wouldn't\", 'myself', \"mightn't\", 'o', 'because', 'other', 'will', 'themselves', 'should', 'why', 'about', 'which', \"hasn't\", 'being', 'an', 'then', 'them', \"mustn't\", 'your', 'now', 'she', 'no', 'didn', 'before', 'he', 'all', 'own', 'll', 'most', 'it', 'is', 's', 'when', 'don', \"couldn't\", 'hadn', 'ma', 'are', 'wouldn', 'very', 'ours', 'you', 'once', 'doing', 'having', \"it's\", 'than', 'does'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords from our paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['Table', '&', 'Apron', '–', 'formerly', 'The', 'Kitchen', 'Table', 'Restaurant', '&', 'Bakery', '–', 'doesn', '’', 't', 'exist', 'to', 'disrupt', 'the', 'scene', '.', 'From', 'the', 'outside', ',', 'it', 'barely', 'stretches', 'the', 'boundaries', 'of', 'what', 'is', 'an', 'already', 'saturated', 'restaurant-cum-bakery', 'scene', '.', 'But', 'none', 'of', 'it', 'matters', '.', 'Because', 'right', 'from', 'its', 'birth', 'in', '2014', ',', 'Table', '&', 'Apron', 'has', 'proven', 'to', 'be', 'a', 'restaurant', 'that', 'has', 'in', 'spades', 'a', 'component', 'so', 'elementary', 'yet', 'so', 'rare', '–', 'heart', '.', 'Through', 'hard', 'work', ',', 'dedication', 'and', 'all', 'the', 'boring', 'old-fashioned', 'virtues', 'of', 'an', 'honest', 'operation', ',', 'owner', 'Marcus', 'Low', 'and', 'his', 'team', 'have', 'carved', 'for', 'us', 'a', 'little', 'treasure', 'in', 'Damansara', 'Kim', '.', '(', 'Credit', 'must', 'also', 'be', 'given', 'to', 'former', 'co-owner', 'Mei', 'Wan', 'Tan', '.', ')', 'The', 'narcissism', 'you', '’', 'll', 'find', 'in', 'so', 'many', 'KL']\n",
      "\n",
      "Filtered Sentence: ['Table', '&', 'Apron', '–', 'formerly', 'The', 'Kitchen', 'Table', 'Restaurant', '&', 'Bakery', '–', '’', 'exist', 'disrupt', 'scene', '.', 'From', 'outside', ',', 'barely', 'stretches', 'boundaries', 'already', 'saturated', 'restaurant-cum-bakery', 'scene', '.', 'But', 'none', 'matters', '.', 'Because', 'right', 'birth', '2014', ',', 'Table', '&', 'Apron', 'proven', 'restaurant', 'spades', 'component', 'elementary', 'yet', 'rare', '–', 'heart', '.', 'Through', 'hard', 'work', ',', 'dedication', 'boring', 'old-fashioned', 'virtues', 'honest', 'operation', ',', 'owner', 'Marcus', 'Low', 'team', 'carved', 'us', 'little', 'treasure', 'Damansara', 'Kim', '.', '(', 'Credit', 'must', 'also', 'given', 'former', 'co-owner', 'Mei', 'Wan', 'Tan', '.', ')', 'The', 'narcissism', '’', 'find', 'many', 'KL']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent = words\n",
    "\n",
    "filtered_sent=[]\n",
    "for w in tokenized_sent:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized Sentence:\",tokenized_sent)\n",
    "print()\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon Normalization\n",
    "\n",
    "Lexicon normalization considers another type of noise in the text. For example, connection, connected, connecting word reduce to a common word \"connect\". It reduces derivationally related forms of a word to a common root word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma. This thing will miss by stemming because it requires a dictionary look-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "#Lexicon Normalization\n",
    "#performing stemming and Lemmatization\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is a process of linguistic normalization, which reduces words to their word root word or chops off the derivational affixes. For example, connection, connected, connecting word reduce to a common word \"connect\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Table', '&', 'Apron', '–', 'formerly', 'The', 'Kitchen', 'Table', 'Restaurant', '&', 'Bakery', '–', '’', 'exist', 'disrupt', 'scene', '.', 'From', 'outside', ',', 'barely', 'stretches', 'boundaries', 'already', 'saturated', 'restaurant-cum-bakery', 'scene', '.', 'But', 'none', 'matters', '.', 'Because', 'right', 'birth', '2014', ',', 'Table', '&', 'Apron', 'proven', 'restaurant', 'spades', 'component', 'elementary', 'yet', 'rare', '–', 'heart', '.', 'Through', 'hard', 'work', ',', 'dedication', 'boring', 'old-fashioned', 'virtues', 'honest', 'operation', ',', 'owner', 'Marcus', 'Low', 'team', 'carved', 'us', 'little', 'treasure', 'Damansara', 'Kim', '.', '(', 'Credit', 'must', 'also', 'given', 'former', 'co-owner', 'Mei', 'Wan', 'Tan', '.', ')', 'The', 'narcissism', '’', 'find', 'many', 'KL']\n",
      "Stemmed Sentence: ['tabl', '&', 'apron', '–', 'formerli', 'the', 'kitchen', 'tabl', 'restaur', '&', 'bakeri', '–', '’', 'exist', 'disrupt', 'scene', '.', 'from', 'outsid', ',', 'bare', 'stretch', 'boundari', 'alreadi', 'satur', 'restaurant-cum-bakeri', 'scene', '.', 'but', 'none', 'matter', '.', 'becaus', 'right', 'birth', '2014', ',', 'tabl', '&', 'apron', 'proven', 'restaur', 'spade', 'compon', 'elementari', 'yet', 'rare', '–', 'heart', '.', 'through', 'hard', 'work', ',', 'dedic', 'bore', 'old-fashion', 'virtu', 'honest', 'oper', ',', 'owner', 'marcu', 'low', 'team', 'carv', 'us', 'littl', 'treasur', 'damansara', 'kim', '.', '(', 'credit', 'must', 'also', 'given', 'former', 'co-own', 'mei', 'wan', 'tan', '.', ')', 'the', 'narciss', '’', 'find', 'mani', 'KL']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "\n",
    "Reference: https://pythonspot.com/nltk-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Table', 'NNP'), ('&', 'CC'), ('Apron', 'NNP'), ('–', 'NNP'), ('formerly', 'RB'), ('The', 'DT'), ('Kitchen', 'NNP'), ('Table', 'NNP'), ('Restaurant', 'NNP'), ('&', 'CC'), ('Bakery', 'NNP'), ('–', 'NNP'), ('doesn', 'NN'), ('’', 'NNP'), ('t', 'VBZ'), ('exist', 'VBP'), ('to', 'TO'), ('disrupt', 'VB'), ('the', 'DT'), ('scene', 'NN'), ('.', '.'), ('From', 'IN'), ('the', 'DT'), ('outside', 'NN'), (',', ','), ('it', 'PRP'), ('barely', 'RB'), ('stretches', 'VBZ'), ('the', 'DT'), ('boundaries', 'NNS'), ('of', 'IN'), ('what', 'WP'), ('is', 'VBZ'), ('an', 'DT'), ('already', 'RB'), ('saturated', 'VBN'), ('restaurant-cum-bakery', 'JJ'), ('scene', 'NN'), ('.', '.'), ('But', 'CC'), ('none', 'NN'), ('of', 'IN'), ('it', 'PRP'), ('matters', 'NNS'), ('.', '.'), ('Because', 'IN'), ('right', 'NN'), ('from', 'IN'), ('its', 'PRP$'), ('birth', 'NN'), ('in', 'IN'), ('2014', 'CD'), (',', ','), ('Table', 'NNP'), ('&', 'CC'), ('Apron', 'NNP'), ('has', 'VBZ'), ('proven', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('a', 'DT'), ('restaurant', 'NN'), ('that', 'WDT'), ('has', 'VBZ'), ('in', 'IN'), ('spades', 'NNS'), ('a', 'DT'), ('component', 'NN'), ('so', 'RB'), ('elementary', 'JJ'), ('yet', 'RB'), ('so', 'RB'), ('rare', 'JJ'), ('–', 'JJ'), ('heart', 'NN'), ('.', '.'), ('Through', 'IN'), ('hard', 'JJ'), ('work', 'NN'), (',', ','), ('dedication', 'NN'), ('and', 'CC'), ('all', 'PDT'), ('the', 'DT'), ('boring', 'JJ'), ('old-fashioned', 'JJ'), ('virtues', 'NNS'), ('of', 'IN'), ('an', 'DT'), ('honest', 'NN'), ('operation', 'NN'), (',', ','), ('owner', 'NN'), ('Marcus', 'NNP'), ('Low', 'NNP'), ('and', 'CC'), ('his', 'PRP$'), ('team', 'NN'), ('have', 'VBP'), ('carved', 'VBN'), ('for', 'IN'), ('us', 'PRP'), ('a', 'DT'), ('little', 'JJ'), ('treasure', 'NN'), ('in', 'IN'), ('Damansara', 'NNP'), ('Kim', 'NNP'), ('.', '.'), ('(', '('), ('Credit', 'NNP'), ('must', 'MD'), ('also', 'RB'), ('be', 'VB'), ('given', 'VBN'), ('to', 'TO'), ('former', 'JJ'), ('co-owner', 'NN'), ('Mei', 'NNP'), ('Wan', 'NNP'), ('Tan', 'NNP'), ('.', '.'), (')', ')'), ('The', 'DT'), ('narcissism', 'NN'), ('you', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('find', 'VBP'), ('in', 'IN'), ('so', 'RB'), ('many', 'JJ'), ('KL', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "# https://cs.nyu.edu/grishman/jet/guide/PennPOS.html\n",
    "\n",
    "# print(nltk.pos_tag(words))\n",
    "\n",
    "print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dostoevsky', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('son', 'NN'), ('of', 'IN'), ('a', 'DT'), ('doctor', 'NN'), ('.', '.'), ('His', 'PRP$'), ('parents', 'NNS'), ('were', 'VBD'), ('very', 'RB'), ('hard-working', 'JJ'), ('and', 'CC'), ('deeply', 'RB'), ('religious', 'JJ'), ('people', 'NNS'), (',', ','), ('but', 'CC'), ('so', 'RB'), ('poor', 'JJ'), ('that', 'IN'), ('they', 'PRP'), ('lived', 'VBD'), ('with', 'IN'), ('their', 'PRP$'), ('five', 'CD'), ('children', 'NNS'), ('in', 'IN'), ('only', 'RB'), ('two', 'CD'), ('rooms', 'NNS'), ('.', '.'), ('The', 'DT'), ('father', 'NN'), ('and', 'CC'), ('mother', 'NN'), ('spent', 'VBN'), ('their', 'PRP$'), ('evenings', 'NNS'), ('in', 'IN'), ('reading', 'VBG'), ('aloud', 'NN'), ('to', 'TO'), ('their', 'PRP$'), ('children', 'NNS'), (',', ','), ('generally', 'RB'), ('from', 'IN'), ('books', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('serious', 'JJ'), ('character', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\" Dostoevsky was the son of a doctor. \n",
    "His parents were very hard-working and deeply religious people,\n",
    "but so poor that they lived with their five children in only\n",
    "two rooms. The father and mother spent their evenings\n",
    "in reading aloud to their children, generally from books of\n",
    "a serious character.\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "import nltk\n",
    "tagged = nltk.pos_tag(words)\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dostoevsky', 'JJ'), ('son', 'NN'), ('doctor', 'NN'), ('parents', 'NNS'), ('deeply', 'RB'), ('religious', 'JJ'), ('people', 'NNS'), ('poor', 'JJ'), ('lived', 'VBD'), ('five', 'CD'), ('children', 'NNS'), ('two', 'CD'), ('rooms', 'NNS'), ('father', 'RB'), ('mother', 'RB'), ('spent', 'JJ'), ('evenings', 'NNS'), ('reading', 'VBG'), ('aloud', 'JJ'), ('children', 'NNS'), ('generally', 'RB'), ('books', 'NNS'), ('serious', 'JJ'), ('character', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "text = \"\"\" Dostoevsky was the son of a doctor. \n",
    "His parents were very hard-working and deeply religious people,\n",
    "but so poor that they lived with their five children in only\n",
    "two rooms. The father and mother spent their evenings\n",
    "in reading aloud to their children, generally from books of\n",
    "a serious character.\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "def is_ok(token):\n",
    "    return re.match('^[a-z]+$', token) and token not in stop_words\n",
    "\n",
    "filtered = [word for word in word_tokenize(text.lower()) if is_ok(word)]\n",
    "\n",
    "import nltk\n",
    "tagged = nltk.pos_tag(filtered)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "\n",
    "print(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT white/JJ dog/NN)\n",
      "  (NP fight/NN)\n",
      "  with/IN\n",
      "  (NP a/DT black/JJ cat/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import chunk\n",
    "\n",
    "text = \"The white dog fight with a black cat\"\n",
    "words = word_tokenize(text)\n",
    "tagged = nltk.pos_tag(words)\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Table/NNP)\n",
      "  &/CC\n",
      "  (PERSON Apron/NNP)\n",
      "  –/NNP\n",
      "  formerly/RB\n",
      "  The/DT\n",
      "  (ORGANIZATION Kitchen/NNP Table/NNP Restaurant/NNP)\n",
      "  &/CC\n",
      "  (PERSON Bakery/NNP)\n",
      "  –/NNP\n",
      "  doesn/NN\n",
      "  ’/NNP\n",
      "  t/VBZ\n",
      "  exist/VBP\n",
      "  to/TO\n",
      "  disrupt/VB\n",
      "  the/DT\n",
      "  scene/NN\n",
      "  ./.\n",
      "  From/IN\n",
      "  the/DT\n",
      "  outside/NN\n",
      "  ,/,\n",
      "  it/PRP\n",
      "  barely/RB\n",
      "  stretches/VBZ\n",
      "  the/DT\n",
      "  boundaries/NNS\n",
      "  of/IN\n",
      "  what/WP\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  already/RB\n",
      "  saturated/VBN\n",
      "  restaurant-cum-bakery/JJ\n",
      "  scene/NN\n",
      "  ./.\n",
      "  But/CC\n",
      "  none/NN\n",
      "  of/IN\n",
      "  it/PRP\n",
      "  matters/NNS\n",
      "  ./.\n",
      "  Because/IN\n",
      "  right/NN\n",
      "  from/IN\n",
      "  its/PRP$\n",
      "  birth/NN\n",
      "  in/IN\n",
      "  2014/CD\n",
      "  ,/,\n",
      "  (PERSON Table/NNP)\n",
      "  &/CC\n",
      "  (PERSON Apron/NNP)\n",
      "  has/VBZ\n",
      "  proven/VBN\n",
      "  to/TO\n",
      "  be/VB\n",
      "  a/DT\n",
      "  restaurant/NN\n",
      "  that/WDT\n",
      "  has/VBZ\n",
      "  in/IN\n",
      "  spades/NNS\n",
      "  a/DT\n",
      "  component/NN\n",
      "  so/RB\n",
      "  elementary/JJ\n",
      "  yet/RB\n",
      "  so/RB\n",
      "  rare/JJ\n",
      "  –/JJ\n",
      "  heart/NN\n",
      "  ./.\n",
      "  Through/IN\n",
      "  hard/JJ\n",
      "  work/NN\n",
      "  ,/,\n",
      "  dedication/NN\n",
      "  and/CC\n",
      "  all/PDT\n",
      "  the/DT\n",
      "  boring/JJ\n",
      "  old-fashioned/JJ\n",
      "  virtues/NNS\n",
      "  of/IN\n",
      "  an/DT\n",
      "  honest/NN\n",
      "  operation/NN\n",
      "  ,/,\n",
      "  owner/NN\n",
      "  (PERSON Marcus/NNP Low/NNP)\n",
      "  and/CC\n",
      "  his/PRP$\n",
      "  team/NN\n",
      "  have/VBP\n",
      "  carved/VBN\n",
      "  for/IN\n",
      "  us/PRP\n",
      "  a/DT\n",
      "  little/JJ\n",
      "  treasure/NN\n",
      "  in/IN\n",
      "  (GPE Damansara/NNP)\n",
      "  (PERSON Kim/NNP)\n",
      "  ./.\n",
      "  (/(\n",
      "  (ORGANIZATION Credit/NNP)\n",
      "  must/MD\n",
      "  also/RB\n",
      "  be/VB\n",
      "  given/VBN\n",
      "  to/TO\n",
      "  former/JJ\n",
      "  co-owner/NN\n",
      "  (PERSON Mei/NNP Wan/NNP Tan/NNP)\n",
      "  ./.\n",
      "  )/)\n",
      "  The/DT\n",
      "  narcissism/NN\n",
      "  you/PRP\n",
      "  ’/VBP\n",
      "  ll/JJ\n",
      "  find/VBP\n",
      "  in/IN\n",
      "  so/RB\n",
      "  many/JJ\n",
      "  KL/NNP)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "def entities(text):\n",
    "    return ne_chunk(\n",
    "        pos_tag(\n",
    "            word_tokenize(text)\n",
    "        )\n",
    "    )\n",
    "\n",
    "tree = entities (text)\n",
    "\n",
    "tree.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tree.draw of Tree('S', [Tree('GPE', [('Table', 'NNP')]), ('&', 'CC'), Tree('PERSON', [('Apron', 'NNP')]), ('–', 'NNP'), ('formerly', 'RB'), ('The', 'DT'), Tree('ORGANIZATION', [('Kitchen', 'NNP'), ('Table', 'NNP'), ('Restaurant', 'NNP')]), ('&', 'CC'), Tree('PERSON', [('Bakery', 'NNP')]), ('–', 'NNP'), ('doesn', 'NN'), ('’', 'NNP'), ('t', 'VBZ'), ('exist', 'VBP'), ('to', 'TO'), ('disrupt', 'VB'), ('the', 'DT'), ('scene', 'NN'), ('.', '.'), ('From', 'IN'), ('the', 'DT'), ('outside', 'NN'), (',', ','), ('it', 'PRP'), ('barely', 'RB'), ('stretches', 'VBZ'), ('the', 'DT'), ('boundaries', 'NNS'), ('of', 'IN'), ('what', 'WP'), ('is', 'VBZ'), ('an', 'DT'), ('already', 'RB'), ('saturated', 'VBN'), ('restaurant-cum-bakery', 'JJ'), ('scene', 'NN'), ('.', '.'), ('But', 'CC'), ('none', 'NN'), ('of', 'IN'), ('it', 'PRP'), ('matters', 'NNS'), ('.', '.'), ('Because', 'IN'), ('right', 'NN'), ('from', 'IN'), ('its', 'PRP$'), ('birth', 'NN'), ('in', 'IN'), ('2014', 'CD'), (',', ','), Tree('PERSON', [('Table', 'NNP')]), ('&', 'CC'), Tree('PERSON', [('Apron', 'NNP')]), ('has', 'VBZ'), ('proven', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('a', 'DT'), ('restaurant', 'NN'), ('that', 'WDT'), ('has', 'VBZ'), ('in', 'IN'), ('spades', 'NNS'), ('a', 'DT'), ('component', 'NN'), ('so', 'RB'), ('elementary', 'JJ'), ('yet', 'RB'), ('so', 'RB'), ('rare', 'JJ'), ('–', 'JJ'), ('heart', 'NN'), ('.', '.'), ('Through', 'IN'), ('hard', 'JJ'), ('work', 'NN'), (',', ','), ('dedication', 'NN'), ('and', 'CC'), ('all', 'PDT'), ('the', 'DT'), ('boring', 'JJ'), ('old-fashioned', 'JJ'), ('virtues', 'NNS'), ('of', 'IN'), ('an', 'DT'), ('honest', 'NN'), ('operation', 'NN'), (',', ','), ('owner', 'NN'), Tree('PERSON', [('Marcus', 'NNP'), ('Low', 'NNP')]), ('and', 'CC'), ('his', 'PRP$'), ('team', 'NN'), ('have', 'VBP'), ('carved', 'VBN'), ('for', 'IN'), ('us', 'PRP'), ('a', 'DT'), ('little', 'JJ'), ('treasure', 'NN'), ('in', 'IN'), Tree('GPE', [('Damansara', 'NNP')]), Tree('PERSON', [('Kim', 'NNP')]), ('.', '.'), ('(', '('), Tree('ORGANIZATION', [('Credit', 'NNP')]), ('must', 'MD'), ('also', 'RB'), ('be', 'VB'), ('given', 'VBN'), ('to', 'TO'), ('former', 'JJ'), ('co-owner', 'NN'), Tree('PERSON', [('Mei', 'NNP'), ('Wan', 'NNP'), ('Tan', 'NNP')]), ('.', '.'), (')', ')'), ('The', 'DT'), ('narcissism', 'NN'), ('you', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('find', 'VBP'), ('in', 'IN'), ('so', 'RB'), ('many', 'JJ'), ('KL', 'NNP')])>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree.draw\n",
    "\n",
    "#from nltk.corpus import treebank\n",
    "#tree = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "tree.draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female\n",
      "0.738\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Data\n",
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')]\n",
    "\t+ [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "#print(labeled_names[:10])\n",
    "\n",
    "# Step 2: Extract last letter of a name as the feature and form feature set\n",
    "def feature_extractor(name):\n",
    "    return {'last_letter': name[-1]}\n",
    "\n",
    "featureset = [(feature_extractor(name), gender) for (name, gender) in labeled_names]\n",
    "# print(featureset[:10])\n",
    "\n",
    "# Step 3: Split the feature set to training/testing datasets\n",
    "\n",
    "train_set, test_set = featureset[500:], featureset[:500]\n",
    "\n",
    "# Step 4/5: Load the classifier and perform training\n",
    "import nltk\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Step 6: Prediction/Evaluation\n",
    "print(classifier.classify(feature_extractor('Danny')))\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 0.84\n",
      "Most Informative Features\n",
      "                   sucks = True              neg : pos    =     10.3 : 1.0\n",
      "                  annual = True              pos : neg    =      9.6 : 1.0\n",
      "                 saddled = True              neg : pos    =      7.7 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.7 : 1.0\n",
      "                 frances = True              pos : neg    =      7.6 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.5 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.3 : 1.0\n",
      "               atrocious = True              neg : pos    =      7.1 : 1.0\n",
      "                  shoddy = True              neg : pos    =      7.1 : 1.0\n",
      "           unimaginative = True              neg : pos    =      7.1 : 1.0\n",
      "                  regard = True              pos : neg    =      6.5 : 1.0\n",
      "                 kidding = True              neg : pos    =      6.4 : 1.0\n",
      "                    mena = True              neg : pos    =      6.4 : 1.0\n",
      "                  suvari = True              neg : pos    =      6.4 : 1.0\n",
      "                  turkey = True              neg : pos    =      6.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Step 1: Load Data \n",
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "import random\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Step 2: Extract Feature\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def feature_extractor(review):\n",
    "    words = set(review)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "    \n",
    "featureset = [(feature_extractor(review), sentiment) for (review, sentiment) in documents]\n",
    "\n",
    "\n",
    "# Step 3: Split the feature set to training/testing datasets\n",
    "training_set, testing_set = featureset[:1900],featureset[1900:]\n",
    "\n",
    "# Step 4/5: Load the classifier and perform training\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# Step 6: Prediction/Evaluation\n",
    "print(\"Classifier accuracy:\",nltk.classify.accuracy(classifier, testing_set))\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "\n",
    "\n",
    "**Installation**\n",
    "\n",
    "conda install -c conda-forge textblob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between TextBlob and NLTK\n",
    "\n",
    "Reference: https://www.quora.com/What-is-the-use-of-NLTK-and-TextBlob-What-is-the-difference-between-both-And-for-text-analysis-which-tool-is-better#:~:text=I%20als-,NLTK%20and%20TextBlob%20are%20both%20excellent%20libraries%20for%20NLP.,built%20upon%20NLTK%20and%20Pattern.&text=It's%20a%20newer%20NLP%20library,POS%20tagging%20than%20NLTK%20does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apron', 'kitchen', 'table restaurant', 'bakery', '– doesn ’ t', 'restaurant-cum-bakery scene', 'apron', 'rare – heart', 'hard work', 'honest operation', 'marcus low', 'damansara kim', 'credit', 'mei wan tan', '’ ll', 'kl']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "## Exploring Features of NLTK:\n",
    "\n",
    "a. Open the text file for processing:\n",
    "First, we are going to open and read the file which we want to analyze\n",
    "eg. The fishing documentation in txt file (page2).\n",
    "Reference: https://huntfish.mdc.mo.gov/sites/default/files/downloads/page/IntroToFishing_2017_v2.pdf\n",
    "\n",
    "b. Import required libraries:\n",
    "For various data processing cases in NLP, we need to import some libraries. In this case, we are going to use NLTK for Natural Language Processing. We will use it to perform various operations on the text.\n",
    "\n",
    "c. Sentence tokenizing:\n",
    "By tokenizing the text with sent_tokenize( ), we can get the text as sentences.\n",
    "\n",
    "d. Word tokenizing:\n",
    "By tokenizing the text with word_tokenize( ), we can get the text as words.\n",
    "\n",
    "e. Find the frequency distribution:\n",
    "Let’s find out the frequency of words in our text.\n",
    "\n",
    "f. Plot the frequency graph:\n",
    "Let’s plot a graph to visualize the word distribution in our text.\n",
    "\n",
    "g. Remove punctuation marks:\n",
    "Next, we are going to remove the punctuation marks as they are not very useful for us. We are going to use isalpha( ) method to separate the punctuation marks from the actual text. Also, we are going to make a new list called words_no_punc, which will store the words in lower case but exclude the punctuation marks.\n",
    "\n",
    "h. Plotting graph without punctuation marks:\n",
    "\n",
    "i. List of stopwords:\n",
    "\n",
    "j. Removing stopwords:\n",
    "\n",
    "k. Final frequency distribution:\n",
    "the final graph has many useful words that help us understand what our sample data is about, showing how essential it is to perform data cleaning on NLP.\n",
    "\n",
    "Hint: https://medium.com/towards-artificial-intelligence/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
